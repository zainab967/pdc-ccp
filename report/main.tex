\documentclass[10pt,twocolumn]{IEEEtran}

% Packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{textcomp}

% Code listing style
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red}
}

\title{Temperature Evolution Simulation of a Rod}

\author{
    \IEEEauthorblockN{Syed Shaheer Nasir}
    \IEEEauthorblockA{Department of Computer Science\\
    University of Management and Technology\\
    Lahore, Pakistan\\
    f2022266454@umt.edu.pk}
    \and
    \IEEEauthorblockN{Zainab Usman}
    \IEEEauthorblockA{Department of Computer Science\\
    University of Management and Technology\\
    Lahore, Pakistan\\
    f2022266828@umt.edu.pk}
}

\begin{document}

\maketitle

% ================================
% ABSTRACT
% ================================
\begin{abstract}
This report presents the implementation and performance analysis of parallel solutions for simulating temperature evolution in a one-dimensional rod using the heat equation. We implement four versions: sequential, OpenMP (shared memory), MPI (distributed memory), and CUDA (GPU). The Forward-Time Central-Space (FTCS) finite difference method is used for numerical discretization. Performance analysis with N=10,000 grid points reveals that OpenMP achieves a 1.74x speedup, while MPI shows modest improvement and CUDA exhibits overhead limitations for this iterative problem. Results are validated against the analytical solution with L2 error of $3.82 \times 10^{-8}$.

\textbf{Keywords:} Heat Equation, Parallel Computing, OpenMP, MPI, CUDA, Finite Difference Method
\end{abstract}

% ================================
% INTRODUCTION
% ================================
\section{Introduction}

The heat equation is a fundamental partial differential equation (PDE) that describes how temperature evolves over time in a material. For a one-dimensional rod, the heat equation is:

\begin{equation}
\frac{\partial T}{\partial t} = \alpha \frac{\partial^2 T}{\partial x^2}
\label{eq:heat}
\end{equation}

where $T(x,t)$ is the temperature at position $x$ and time $t$, and $\alpha$ is the thermal diffusivity of the material.

\subsection{Problem Statement}

We simulate temperature evolution of a rod with:
\begin{itemize}
    \item Rod length: $L = 1$ meter
    \item Boundary conditions: $T(0,t) = T(L,t) = 0^\circ$C
    \item Initial condition: $T(x,0) = 100\sin(\pi x)$
    \item Thermal diffusivity: $\alpha = 0.01$ m$^2$/s
\end{itemize}

% ================================
% METHODOLOGY
% ================================
\section{Methodology}

\subsection{Numerical Method}

We use the Forward-Time Central-Space (FTCS) explicit finite difference scheme:

\begin{equation}
T_i^{n+1} = T_i^n + r(T_{i+1}^n - 2T_i^n + T_{i-1}^n)
\label{eq:ftcs}
\end{equation}

where $r = \frac{\alpha \Delta t}{(\Delta x)^2}$ is the diffusion number. For numerical stability, the CFL condition requires $r \leq 0.5$.

\subsection{Analytical Solution}

The exact solution for validation is:

\begin{equation}
T(x,t) = 100 e^{-\alpha \pi^2 t} \sin(\pi x)
\label{eq:analytical}
\end{equation}

% ================================
% IMPLEMENTATION
% ================================
\section{Implementation}

\subsection{Sequential Implementation}

The baseline sequential implementation iterates through all spatial grid points at each time step. Algorithm~\ref{alg:seq} shows the main loop structure.

\begin{algorithm}
\caption{Sequential FTCS}
\label{alg:seq}
\begin{algorithmic}[1]
\State Initialize $T$ with initial condition
\For{each time step}
    \For{$i = 1$ to $N-2$}
        \State $T_{new}[i] \gets T[i] + r(T[i+1] - 2T[i] + T[i-1])$
    \EndFor
    \State Swap $T$ and $T_{new}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{OpenMP Implementation}

OpenMP parallelizes the spatial loop using the \texttt{\#pragma omp parallel for} directive. Since there are no data dependencies within a single time step, this is an embarrassingly parallel operation.

\begin{lstlisting}[language=C++]
#pragma omp parallel for
for (int i = 1; i < N-1; i++) {
    T_new[i] = T_old[i] + r * 
        (T_old[i+1] - 2*T_old[i] + T_old[i-1]);
}
\end{lstlisting}

\subsection{MPI Implementation}

MPI uses domain decomposition to distribute the spatial domain across processes. Each process computes a local portion and exchanges ghost cells with neighbors using \texttt{MPI\_Sendrecv}.

\subsection{CUDA Implementation}

The CUDA implementation assigns one GPU thread per grid point. The kernel computes the update for each interior point in parallel.

\begin{lstlisting}[language=C++]
__global__ void heat_kernel(double* T_old, 
                            double* T_new, 
                            int N, double r) {
    int i = blockIdx.x*blockDim.x + threadIdx.x;
    if (i > 0 && i < N-1) {
        T_new[i] = T_old[i] + r * 
            (T_old[i+1] - 2*T_old[i] + T_old[i-1]);
    }
}
\end{lstlisting}

% ================================
% RESULTS
% ================================
\section{Results and Performance Analysis}

\subsection{Experimental Setup}

\begin{itemize}
    \item CPU: Intel Core (4 threads used for testing)
    \item GPU: NVIDIA Quadro T1000
    \item Grid points: $N = 10,000$
    \item Time steps: 1,249,750
    \item Simulation time: 0.5 seconds
\end{itemize}

\subsection{Performance Comparison}

\begin{table}[h]
\centering
\caption{Execution Time Comparison (N=10,000)}
\label{tab:performance}
\begin{tabular}{@{}lccc@{}}
\toprule
Implementation & Threads/Procs & Time (ms) & Speedup \\
\midrule
Sequential     & 1             & 6,079.44  & 1.00x   \\
\textbf{OpenMP}& \textbf{4}   & \textbf{3,485.92} & \textbf{1.74x} \\
MPI            & 4             & 5,259.89  & 1.16x   \\
CUDA           & GPU           & 11,146.24 & 0.55x   \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{execution_time_chart.png}
\caption{Execution Time Comparison across implementations}
\label{fig:exectime}
\end{figure}

\subsection{Speedup Analysis}

According to Amdahl's Law, the theoretical speedup is limited by:

\begin{equation}
S(p) = \frac{1}{(1-P) + \frac{P}{p}}
\end{equation}

where $P$ is the parallel fraction and $p$ is the number of processors.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{speedup_chart.png}
\caption{Speedup Analysis relative to Sequential baseline}
\label{fig:speedup}
\end{figure}

\subsection{Performance Discussion}

Our results show that OpenMP achieves the best performance for this problem. MPI shows modest speedup while CUDA exhibits overhead limitations:

\begin{itemize}
    \item \textbf{OpenMP (1.74x speedup):} Low thread synchronization overhead makes it ideal for this compute-bound problem.
    \item \textbf{MPI (1.16x speedup):} Shows modest improvement with 4 processes. The overhead of inter-process communication on a single machine limits scaling, but still achieves speedup through parallelization.
    \item \textbf{CUDA (0.55x):} With 1.25 million kernel launches (one per time step), kernel launch overhead ($\sim$5-10$\mu$s each) accumulates significantly, outweighing GPU parallelism benefits.
\end{itemize}

This demonstrates that the optimal parallelization strategy depends on problem characteristics, not just available hardware.

\subsection{Numerical Accuracy}

All implementations produce identical results when compared to the analytical solution:

\begin{itemize}
    \item L2 Error: $3.82 \times 10^{-8}$ (all implementations)
    \item Maximum temperature at $t=0.5$s: 95.18Â°C
\end{itemize}

The higher accuracy with N=10,000 (vs N=100) demonstrates improved numerical precision with finer spatial discretization.

% ================================
% CONCLUSION
% ================================
\section{Conclusion}

This project successfully demonstrates parallel computing techniques for solving the heat equation. Key findings:

\begin{itemize}
    \item \textbf{OpenMP achieved 1.74x speedup} with 4 threads, making it the best choice for this iterative problem on shared-memory systems
    \item \textbf{MPI achieved 1.16x speedup} with 4 processes on a single machine, demonstrating that parallelization benefits can outweigh communication overhead even in non-distributed setups
    \item \textbf{CUDA requires problem restructuring} to minimize kernel launches; batch processing or implicit methods would improve GPU utilization
    \item All implementations maintain identical numerical accuracy ($L2 = 3.82 \times 10^{-8}$)
\end{itemize}

The project demonstrates that selecting the right parallelization approach requires understanding both the algorithm structure and target hardware characteristics.

% ================================
% REFERENCES
% ================================
\begin{thebibliography}{9}

\bibitem{openmp}
OpenMP Architecture Review Board, "OpenMP Application Programming Interface," Version 5.0, 2018.

\bibitem{mpi}
Message Passing Interface Forum, "MPI: A Message-Passing Interface Standard," Version 3.1, 2015.

\bibitem{cuda}
NVIDIA Corporation, "CUDA C++ Programming Guide," Version 12.0, 2023.

\bibitem{numerical}
J. W. Thomas, "Numerical Partial Differential Equations: Finite Difference Methods," Springer, 1995.

\end{thebibliography}

% ================================
% AUTHOR BIOS
% ================================
\section*{Author Biographies}

% TODO: Add author photos
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.15\textwidth]{author1.jpg}
% \end{figure}

\textbf{Syed Shaheer Nasir} is a Computer Science student at UMT Lahore with an interest in artificial intelligence and machine learning models.

\textbf{Zainab Usman} is a Computer Science student at UMT Lahore with interests in machine learning and LLMs.

\vspace{1em}

\textbf{Overleaf Link:} \url{https://www.overleaf.com/read/hyqzgndjpvww#669d84}

\end{document}
